{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "SECTION 1 — Downloading NLP Corpus and Required NLTK Resources\n",
        "\n",
        "Natural Language Toolkit (NLTK) provides many built-in corpora that allow us to practice real NLP tasks.\n",
        "In this section, we download the Movie Reviews corpus and also install essential tokenizers, stopwords, and WordNet for lemmatization."
      ],
      "metadata": {
        "id": "QySiE7lNcVFU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('movie_reviews')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download(\"punkt_tab\")\n",
        "\n",
        "\n",
        "from nltk.corpus import movie_reviews\n",
        "\n",
        "# Load corpus into a single text string\n",
        "text = \" \".join(movie_reviews.words())\n",
        "\n",
        "print(\"Sample Original Text:\\n\", text[:300])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wqFmRw7iccl3",
        "outputId": "a1e319de-b0c1-41ce-815b-284076b11321"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample Original Text:\n",
            " plot : two teen couples go to a church party , drink and then drive . they get into an accident . one of the guys dies , but his girlfriend continues to see him in her life , and has nightmares . what ' s the deal ? watch the movie and \" sorta \" find out . . . critique : a mind - fuck movie for the \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SECTION 2 — Tokenization (Splitting Text into Words & Sentences)\n",
        "\n",
        "Tokenization is the process of breaking a large text into smaller meaningful units.\n",
        "\n",
        "Word tokenization splits text into individual words.\n",
        "\n",
        "Sentence tokenization splits text into complete sentences.\n",
        "\n",
        "This step is the foundation of all NLP processing, because every further technique—cleaning, stemming, lemmatizing—operates on tokens."
      ],
      "metadata": {
        "id": "lX5_E_XNgIc9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJm-Zu7JcT1-",
        "outputId": "e058e377-c704-4097-8832-d849d2597298"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 20 word tokens:\n",
            " ['plot', ':', 'two', 'teen', 'couples', 'go', 'to', 'a', 'church', 'party', ',', 'drink', 'and', 'then', 'drive', '.', 'they', 'get', 'into', 'an']\n",
            "\n",
            "First 2 sentence tokens:\n",
            " ['plot : two teen couples go to a church party , drink and then drive .', 'they get into an accident .']\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "word_tokens = word_tokenize(text)\n",
        "sent_tokens = sent_tokenize(text)\n",
        "\n",
        "print(\"First 20 word tokens:\\n\", word_tokens[:20])\n",
        "print(\"\\nFirst 2 sentence tokens:\\n\", sent_tokens[:2])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SECTION 3 — Regular Expression (Regex) Cleaning\n",
        "\n",
        "Raw text contains punctuation, numbers, symbols, and uppercase/lowercase variations.\n",
        "To prepare text for analysis, we clean it using Regular Expressions (regex), which help remove unwanted characters.\n",
        "Here’s what we do:\n",
        "\n",
        "Convert text to lowercase\n",
        "\n",
        "Remove punctuation and numbers\n",
        "\n",
        "Remove extra spaces\n",
        "\n",
        "This creates a neat, uniform text for further NLP processing."
      ],
      "metadata": {
        "id": "6KE9FldOgN5l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "clean_text = text.lower()                                     # lowercase\n",
        "clean_text = re.sub(r'[^a-z\\s]', ' ', clean_text)             # keep only alphabets\n",
        "clean_text = re.sub(r'\\s+', ' ', clean_text).strip()          # remove extra spaces\n",
        "\n",
        "print(\"Cleaned Text Preview:\\n\", clean_text[:300])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kq8RIF2Net_h",
        "outputId": "93e76135-99d5-4803-ddc4-265e4663968e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned Text Preview:\n",
            " plot two teen couples go to a church party drink and then drive they get into an accident one of the guys dies but his girlfriend continues to see him in her life and has nightmares what s the deal watch the movie and sorta find out critique a mind fuck movie for the teen generation that touches on \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SECTION 4 — Removing Stopwords\n",
        "\n",
        "Stopwords are common English words like the, is, of, and, you, which do not contribute much meaning.\n",
        "Removing them helps focus on important content words such as movie, film, action, actor, etc.\n",
        "\n",
        "We use NLTK's built-in stopwords list and remove all stopwords from our cleaned tokens."
      ],
      "metadata": {
        "id": "89Cb_HOJgSww"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Section 4 — Stopwords Removal\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "tokens_clean = clean_text.split()\n",
        "filtered_tokens = [w for w in tokens_clean if w not in stop_words]\n",
        "\n",
        "print(\"Before Stopwords Count:\", len(tokens_clean))\n",
        "print(\"After Stopwords Count:\", len(filtered_tokens))\n",
        "print(\"Sample tokens:\", filtered_tokens[:20])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvC6HHIGe5O-",
        "outputId": "e25f4a67-116e-4927-96d0-bf25810b3f61"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before Stopwords Count: 1331272\n",
            "After Stopwords Count: 702479\n",
            "Sample tokens: ['plot', 'two', 'teen', 'couples', 'go', 'church', 'party', 'drink', 'drive', 'get', 'accident', 'one', 'guys', 'dies', 'girlfriend', 'continues', 'see', 'life', 'nightmares', 'deal']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SECTION 5 — Word Count (Frequency Distribution)\n",
        "\n",
        "Word frequency analysis tells us which words appear most often in the corpus.\n",
        "This is important for text summarization, keyword extraction, and linguistic observation."
      ],
      "metadata": {
        "id": "cpUpUVspgWwJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import FreqDist\n",
        "\n",
        "freq = FreqDist(filtered_tokens)\n",
        "\n",
        "print(\"Top 20 most common words:\\n\")\n",
        "for word, count in freq.most_common(20):\n",
        "    print(f\"{word} → {count}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gQ8nGAF7fCxN",
        "outputId": "05f88f8a-24bc-49fc-816a-b1dadd9d284a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 20 most common words:\n",
            "\n",
            "film → 9519\n",
            "one → 5854\n",
            "movie → 5775\n",
            "like → 3691\n",
            "even → 2565\n",
            "good → 2411\n",
            "time → 2411\n",
            "story → 2170\n",
            "would → 2110\n",
            "much → 2050\n",
            "character → 2020\n",
            "also → 1967\n",
            "get → 1949\n",
            "two → 1912\n",
            "well → 1906\n",
            "characters → 1859\n",
            "first → 1836\n",
            "see → 1749\n",
            "way → 1693\n",
            "make → 1642\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SECTION 6 — Stemming\n",
        "Stemming reduces words to their root form by removing suffixes.\n",
        "For example:\n",
        "\n",
        "running → run\n",
        "\n",
        "movies → movi\n",
        "\n",
        "It is a fast, rule-based process.\n",
        "We use the Porter Stemmer, one of the most common stemming algorithms."
      ],
      "metadata": {
        "id": "kgvl5jf8gb4y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "stemmed_tokens = [stemmer.stem(w) for w in filtered_tokens]\n",
        "\n",
        "print(\"Original vs Stemmed:\")\n",
        "for i in range(10):\n",
        "    print(filtered_tokens[i], \"→\", stemmed_tokens[i])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P63WKXQtfKi5",
        "outputId": "2438000f-f370-4127-bf88-486908895736"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original vs Stemmed:\n",
            "plot → plot\n",
            "two → two\n",
            "teen → teen\n",
            "couples → coupl\n",
            "go → go\n",
            "church → church\n",
            "party → parti\n",
            "drink → drink\n",
            "drive → drive\n",
            "get → get\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SECTION 7 — Lemmatization\n",
        "\n",
        "Lemmatization is smarter than stemming because it converts words to their dictionary base form (lemma).\n",
        "It considers grammar and vocabulary, making the output clean and meaningful.\n",
        "Examples:\n",
        "\n",
        "better → good\n",
        "\n",
        "studies → study\n",
        "\n",
        "cars → car\n",
        "\n",
        "We use WordNetLemmatizer, which uses WordNet’s lexical database."
      ],
      "metadata": {
        "id": "Mi7RzQIYgfrV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "lemm_tokens = [lemmatizer.lemmatize(w) for w in filtered_tokens]\n",
        "\n",
        "print(\"Original vs Lemmatized:\")\n",
        "for i in range(10):\n",
        "    print(filtered_tokens[i], \"→\", lemm_tokens[i])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_GGvPvVzfZif",
        "outputId": "38a65e88-975e-452c-f70d-923ea63cb55b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original vs Lemmatized:\n",
            "plot → plot\n",
            "two → two\n",
            "teen → teen\n",
            "couples → couple\n",
            "go → go\n",
            "church → church\n",
            "party → party\n",
            "drink → drink\n",
            "drive → drive\n",
            "get → get\n"
          ]
        }
      ]
    }
  ]
}